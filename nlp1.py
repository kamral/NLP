'''
НЛП со спа-центром
spaCy - ведущая библиотека для НЛП, и она быстро стала одной из самых популярных фреймворков Python.
Большинство людей находят его интуитивно понятным и снабженным отличной документацией.
spaCy полагается на модели, которые зависят от языка и бывают разных размеров.
Вы можете загрузить модель spaCy с помощью spacy.load.
Например, вот как вы загрузите англоязычную модель.
'''
import spacy
nlp=spacy.load('en_core_web_sm')

'''
После загрузки модели вы можете обрабатывать текст следующим образом:
'''

doc=nlp('Tea is healthy and calming, don not you think?')
'''

Токенизация
Это возвращает объект документа, содержащий токены. 
Маркер - это единица текста в документе, например отдельные слова и знаки препинания. 
SpaCy разделяет сокращения, такие как «не», на два токена «делать» и «нет». 
Вы можете увидеть токены, просматривая документ.
'''

for token in doc:
    print(token)
'''
Tea
is
healthy
and
calming
,
don
not
you
think
?

'''
'''
Итерация по документу дает вам объекты-токены. 
Каждый из этих токенов содержит дополнительную информацию. 
В большинстве случаев важными являются token.lemma_ и token.is_stop.
'''

'''
Предварительная обработка текста
Есть несколько типов предварительной обработки, чтобы улучшить то,
 как мы моделируем с помощью слов. 
Первый - «лемматизация». «Лемма» слова - это его основная форма.
 Например, «прогулка» - это лемма слова «ходьба». 
 Итак, когда вы лемматизируете слово «ходьба», вы преобразовываете его в «ходьба».

Также часто удаляются игнорируемые слова.
 Стоп-слова - это слова, которые часто встречаются в языке и не содержат много информации. 
 Английские стоп-слова включают «the», «is», «and», «but», «not».
С токеном spaCy token.lemma_ возвращает лемму, 
а token.is_stop возвращает логическое значение True, 
если токен является стоп-словом (и False в противном случае).
'''

print(f"Token \t\tLemma \t\tStopword".format('Token', 'Lemma', 'Stopword'))
print("-"*40)
for token in doc:
    print(f"{str(token)}\t\t{token.lemma_}\t\t{token.is_stop}")

'''
Token           Lemma           Stopword
----------------------------------------
Tea             tea             False
is              be              True
healthy         healthy         False
and             and             True
calming         calm            False
,               ,               False
don             don             False
not             not             True
you             you             True
think           think           False
?               ?               False

'''

'''
Почему важны леммы и определение стоп-слов? В языковых данных много шума,
 смешанного с информативным содержанием. 
В предложенном выше предложении важными словами являются чай, полезный и успокаивающий.
 Удаление стоп-слов может помочь прогнозной модели сосредоточиться на релевантных словах.
  Аналогичным образом помогает лемматизация, комбинируя несколько форм одного и того же слова 
  в одну базовую форму («успокаивающий», «успокаивающий», «успокаивающий» - все изменится на «успокаивающий»).

Однако лемматизация и отбрасывание игнорируемых слов могут привести
 к ухудшению работы ваших моделей. Поэтому вам следует рассматривать 
 эту предварительную обработку как часть процесса оптимизации гиперпараметров.
'''
'''
Соответствие шаблону
Другая распространенная задача НЛП - сопоставление лексем или фраз внутри 
фрагментов текста или целых документов. Сопоставление с образцом можно выполнять
 с помощью регулярных выражений, но возможности сопоставления spaCy, как правило, проще в использовании.

Чтобы сопоставить отдельные токены, вы создаете сопоставление.
 Если вы хотите сопоставить список терминов, проще и эффективнее использовать PhraseMatcher. 
 Например, если вы хотите узнать, где в тексте отображаются различные модели смартфонов, 
вы можете создать шаблоны для названий интересующих моделей. Сначала вы создаете сам PhraseMatcher.
'''

from spacy.matcher import PhraseMatcher

matcher=PhraseMatcher(nlp.vocab, attr='LOWER')
print(matcher)

# <spacy.matcher.phrasematcher.PhraseMatcher object at 0x7fcfe3b0fc10>

'''
Сопоставитель создается с использованием словаря вашей модели. 
Здесь мы используем небольшую английскую модель, которую вы загрузили ранее.
 Установка attr = 'LOWER' будет соответствовать фразам в нижнем регистре. 
 Это обеспечивает сопоставление без учета регистра.
 Затем вы создаете список терминов, которые должны совпадать в тексте.
  Средству сопоставления фраз нужны шаблоны как объекты документа. 
Самый простой способ получить их - составить список с использованием модели nlp.
'''

terms=['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']
patterns=[nlp(text) for text in terms]
print(patterns)
# ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']
matcher.add('TerminologyList', patterns)
'''
Затем вы создаете документ из текста для поиска и используете средство сопоставления фраз, 
чтобы найти, где в тексте встречаются термины.
'''

text_doc = nlp("Glowing review overall photography tests pitting the iPhone 11 Pro against the Galaxy Note 10 Plus and last iPhone XS and Google Pixel 3")
matches=matcher(text_doc)
print(matches)

'''
[(3766102292120407359, 7, 9),
 (3766102292120407359, 12, 14),
  (3766102292120407359, 18, 20),
   (3766102292120407359, 21, 23)
   ]

'''

'''
Здесь совпадения представляют собой кортеж из идентификатора совпадения и позиций начала и конца фразы.
'''

match_id, start, end=matches[1]
print(nlp.vocab.strings[match_id], text_doc[start:end])
# TerminologyList iPhone 11

